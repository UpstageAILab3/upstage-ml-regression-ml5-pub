{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92WHLz8346Yf"
   },
   "source": [
    "# 서울 공동주택 실거래가 예측\n",
    "\n",
    "## Contents\n",
    "- Library Import\n",
    "- Data Load\n",
    "- Data Preprocessing\n",
    "- Feature Engineering\n",
    "- Model Training\n",
    "- Inference\n",
    "- Output File Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-DJzJjvO88V"
   },
   "source": [
    "## 1. Library Import\n",
    "- 필요한 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-NiCLGs4ZpM"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/font/NanumGothic.otf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPJvYT0OPAWS"
   },
   "source": [
    "## 2. Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwnwnpNJeR1"
   },
   "source": [
    "#### 2.1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx6nk3N_4_Tk"
   },
   "outputs": [],
   "source": [
    "# 필요한 데이터를 load 하겠습니다. 경로는 환경에 맞게 지정해주면 됩니다.\n",
    "train_path = './data/train.csv'\n",
    "test_path  = './data/test.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6tWMND66vT8",
    "outputId": "e0ba702b-6662-4723-c5a1-600d7236e597"
   },
   "outputs": [],
   "source": [
    "# Train data와 Test data shape은 아래와 같습니다.\n",
    "print('Train data shape : ', train_df.shape, 'Test data shape : ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVjw5Cnz24nz"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X24Y1OIR2_oa"
   },
   "source": [
    "- 모델링 전에 데이터 내 결측치, 이상치 등을 제거하고 범주형과 연속형 변수를 살펴보도록 하겠습니다!\n",
    "- 먼저, 용이한 전처리를 위해 train과 test data를 합친 하나의 데이터로 진행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vltXVoeRhZiI"
   },
   "outputs": [],
   "source": [
    "# train/test 구분을 위한 칼럼을 하나 만들어 줍니다.\n",
    "train_df['is_test'] = 0\n",
    "test_df['is_test'] = 1\n",
    "concat = pd.concat([train_df, test_df])     # 하나의 데이터로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-iZAY9WxQLK",
    "outputId": "ee049806-cb94-4336-f376-6a5c6947f79a"
   },
   "outputs": [],
   "source": [
    "concat['is_test'].value_counts()      # train과 test data가 하나로 합쳐진 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA를 통해 사용 결정한 필드들만 뽑아서 새로운 DataFrame 생성\n",
    "selected_fields = [\n",
    "    '시군구', '번지', '본번', '부번', '도로명', '아파트명',\n",
    "    '전용면적(㎡)', '계약년월', '계약일', '층', '건축년도',\n",
    "    'target', \n",
    "    'is_test'\n",
    "    ]\n",
    "\n",
    "concat_selected_df = concat[selected_fields].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = concat_selected_df.rename(columns={'전용면적(㎡)':'전용면적'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df['본번'] = concat_selected_df['본번'].astype('str')\n",
    "concat_selected_df['부번'] = concat_selected_df['부번'].astype('str')\n",
    "\n",
    "concat_selected_df['target'] = concat_selected_df['target'].fillna(0)\n",
    "concat_selected_df['target'] = concat_selected_df['target'].astype('int64')\n",
    "\n",
    "concat_selected_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지하철 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subway_path = './data/train_with_subway_infos.csv'\n",
    "train_subway_df = pd.read_csv(train_subway_path)\n",
    "print(train_subway_df.shape)\n",
    "print(train_subway_df.info())\n",
    "print(train_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subway_path  = './data/test_with_subway_infos.csv'\n",
    "test_subway_df = pd.read_csv(test_subway_path)\n",
    "print(test_subway_df.shape)\n",
    "print(test_subway_df.info())\n",
    "print(test_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_subway_df = pd.concat([train_subway_df, test_subway_df], axis=0) \n",
    "print(total_subway_df.shape)\n",
    "print(total_subway_df.info())\n",
    "print(total_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = pd.concat([concat_selected_df, total_subway_df], axis=1) \n",
    "print(concat_selected_df.shape)\n",
    "print(concat_selected_df.info())\n",
    "print(concat_selected_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버스 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bus_path = './data/train_with_bus_infos.csv'\n",
    "train_bus_df = pd.read_csv(train_bus_path)\n",
    "print(train_bus_df.shape)\n",
    "print(train_bus_df.info())\n",
    "print(train_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bus_path  = './data/test_with_bus_infos.csv'\n",
    "test_bus_df = pd.read_csv(test_bus_path)\n",
    "print(test_bus_df.shape)\n",
    "print(test_bus_df.info())\n",
    "print(test_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bus_df = pd.concat([train_bus_df, test_bus_df], axis=0) \n",
    "print(total_bus_df.shape)\n",
    "print(total_bus_df.info())\n",
    "print(total_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지하철 정보에서 이미 머지되었으므로 버스에서는 삭제\n",
    "del total_bus_df['좌표X_2']\n",
    "del total_bus_df['좌표Y_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = pd.concat([concat_selected_df, total_bus_df], axis=1) \n",
    "print(concat_selected_df.shape)\n",
    "print(concat_selected_df.info())\n",
    "print(concat_selected_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXfXRevr3dfe"
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytuwazY43gwQ"
   },
   "source": [
    "- 이제 위에서 만든 파생변수들과 정제한 데이터를 기반으로 본격적으로 부동산 실거래가를 예측하는 모델링을 진행하겠습니다.\n",
    "- 모델링에는 LGBM을 이용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdHNkPO_3tSb"
   },
   "source": [
    "### 5.1. 변수 Encoding\n",
    "- 범주형 변수는 그대로 모델에 투입하면, 모델이 제대로 작동할 수 없습니다.\n",
    "- 따라서 **레이블 인코딩 과정**을 통해 범주형 변수들을 numeric하게 바꾸는 인코딩 과정을 진행해주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCW2dTz12VMZ",
    "outputId": "5c81b9e1-0462-421e-cb77-6b056532e5cd"
   },
   "outputs": [],
   "source": [
    "continuous_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for column in concat_selected_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_selected_df[column]):\n",
    "        continuous_features.append(column)\n",
    "    else:\n",
    "        categorical_features.append(column)\n",
    "\n",
    "continuous_features.remove('target')\n",
    "continuous_features.remove('is_test')\n",
    "\n",
    "print(\"연속형 변수:\", continuous_features)\n",
    "print(\"범주형 변수:\", categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수 인코딩\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    concat_selected_df[col] = le.fit_transform(concat_selected_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_features:\n",
    "    print(f\"{col}: {concat_selected_df[col].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 연속형 변수 인코딩\n",
    "\n",
    "# 1. StandardScaler\n",
    "scaler = StandardScaler()\n",
    "concat_selected_df[continuous_features] = scaler.fit_transform(concat_selected_df[continuous_features])\n",
    "\n",
    "# 2. MinMaxScaler (선택적)\n",
    "# scaler = MinMaxScaler()\n",
    "# df[continuous_features] = scaler.fit_transform(df[continuous_features])\n",
    "\n",
    "# 3. RobustScaler (선택적)\n",
    "# scaler = RobustScaler()\n",
    "# df[continuous_features] = scaler.fit_transform(df[continuous_features])\n",
    "\n",
    "# 4. 로그 변환 (양수 값에 대해서만 적용 가능)\n",
    "# for feature in continuous_features:\n",
    "#     if (concat_selected_df[feature] > 0).all():\n",
    "#         concat_selected_df[f'{feature}_log'] = np.log1p(concat_selected_df[feature])\n",
    "\n",
    "# # 5. 이산화 (예: '전용면적'을 구간으로 나누기)\n",
    "# concat_selected_df['전용면적_구간'] = pd.cut(concat_selected_df['전용면적'], bins=5, labels=['매우작음', '작음', '중간', '큼', '매우큼'])\n",
    "\n",
    "# # 6. 다항식 특성 (선택적)\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# poly_features = poly.fit_transform(concat_selected_df[continuous_features])\n",
    "# poly_features_names = poly.get_feature_names_out(continuous_features)\n",
    "# df_poly = pd.DataFrame(poly_features, columns=poly_features_names)\n",
    "# df = pd.concat([concat_selected_df, df_poly], axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "# print(df[continuous_features + [f'{feature}_log' for feature in continuous_features] + ['전용면적_구간']].head())\n",
    "# print(df_poly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "-mMaaemI6DCI",
    "outputId": "67139863-5f67-4288-fb9a-b6f5a57a7e86"
   },
   "outputs": [],
   "source": [
    "concat_selected_df.head(1)        # 인코딩이 된 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 다시 train과 test dataset을 분할해줍니다. 위에서 제작해 놓았던 is_test 칼럼을 이용합니다.\n",
    "dt_train = concat_selected_df.query('is_test==0')\n",
    "dt_test = concat_selected_df.query('is_test==1')\n",
    "\n",
    "# 이제 is_test 칼럼은 drop해줍니다.\n",
    "dt_train.drop(['is_test'], axis = 1, inplace=True)\n",
    "dt_test.drop(['is_test'], axis = 1, inplace=True)\n",
    "print(dt_train.shape, dt_test.shape)\n",
    "\n",
    "dt_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4sHljC3NWje"
   },
   "source": [
    "### 5.2. Model Training\n",
    "- 위 데이터를 이용해 모델을 train 해보겠습니다. 모델은 LightGBM 이용하겠습니다.\n",
    "- Train과 Valid dataset을 분할하는 과정에서는 `holdout` 방법을 사용하겠습니다. 이 방법의 경우  대략적인 성능을 빠르게 확인할 수 있다는 점에서 baseline에서 사용해보도록 하겠습니다.\n",
    "  - 이 후 추가적인 eda를 통해서 평가세트와 경향을 맞추거나 kfold와 같은 분포에 대한 고려를 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S-ow8vVP_YZ"
   },
   "outputs": [],
   "source": [
    "assert dt_train.shape[1] == dt_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXP9IzrZaBMG"
   },
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y_train = dt_train['target']\n",
    "X_train = dt_train.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGNHMAD2aBGI",
    "outputId": "5471313e-6838-44d3-8567-940c7456ef9e"
   },
   "outputs": [],
   "source": [
    "# LightGBM 모델 정의\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_2ricpfCWti"
   },
   "source": [
    "- 랜덤포레스트의 하이퍼파라미터도 데이터에 맞게 지정해줄 수 있습니다. 데이터에 맞는 하이퍼파라미터를 찾는 것도 성능 향상에 도움이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39uvsgeXey3R",
    "outputId": "139e341d-2abd-4923-ed05-a28a70a4caef"
   },
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대한 예측\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측\n",
    "pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jhh_fZ0DHOd"
   },
   "source": [
    "- 변수 중요도도 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "VbgCRxFgdFQb",
    "outputId": "f9114f72-78e1-471c-fc0b-ba5b8b6b6d2f"
   },
   "outputs": [],
   "source": [
    "# 특성 중요도 출력\n",
    "importance = model.feature_importances_\n",
    "feature_names = model.feature_name_\n",
    "for name, importance in sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{name}: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vycdrTEAa2va"
   },
   "outputs": [],
   "source": [
    "model_file_path = os.path.join('model', f'{train_time}.pkl')\n",
    "\n",
    "# 학습된 모델을 저장합니다. Pickle 라이브러리를 이용하겠습니다.\n",
    "with open(model_file_path, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YncDzsK1zl2w"
   },
   "source": [
    "### 5.4. 평가 및 분석\n",
    "\n",
    "- 훈련, 검증, 테스트 세트에 대해 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import evaluator\n",
    "\n",
    "# get [interpretation, rmse, r2, mae]\n",
    "train_result = evaluator.evaluate_set(y_train, y_train_pred, \"Train\")\n",
    "val_result = evaluator.evaluate_set(y_val, pred, \"Valid\")\n",
    "\n",
    "comprehensive_report = evaluator.comprehensive_evaluation(train_result, val_result)\n",
    "print(comprehensive_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xViv0o6DXQ-"
   },
   "outputs": [],
   "source": [
    "# Validation dataset에 target과 pred 값을 채워주도록 하겠습니다.\n",
    "X_val['target'] = y_val\n",
    "X_val['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D8NCLkWC-g6"
   },
   "outputs": [],
   "source": [
    "# Squared_error를 계산하는 함수를 정의하겠습니다.\n",
    "def calculate_se(target, pred):\n",
    "    squared_errors = (target - pred) ** 2\n",
    "    return squared_errors\n",
    "\n",
    "# RMSE 계산\n",
    "squared_errors = calculate_se(X_val['target'], X_val['pred'])\n",
    "X_val['error'] = squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXbdC7b9HDDQ"
   },
   "outputs": [],
   "source": [
    "# Error가 큰 순서대로 sorting 해 보겠습니다.\n",
    "X_val_sort = X_val.sort_values(by='error', ascending=False)       # 내림차순 sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "gOc8HmvDHhz8",
    "outputId": "17e408cb-9533-4d3b-baf6-245de0ae1a43"
   },
   "outputs": [],
   "source": [
    "X_val_sort.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0Ga4ljBNYIy"
   },
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "7LC7NuRaD_Dd",
    "outputId": "c2827163-dbdd-4c8c-d35b-1c325b8d14c0"
   },
   "outputs": [],
   "source": [
    "dt_test.head(2)      # test dataset에 대한 inference를 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HXvn8ZSa1kt"
   },
   "outputs": [],
   "source": [
    "# 저장된 모델을 불러옵니다.\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbZ2A49LP_T9",
    "outputId": "89676c9b-c0a2-4951-84f0-430c5648331c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = dt_test.drop(['target'], axis=1)\n",
    "\n",
    "# Test dataset에 대한 inference를 진행합니다.\n",
    "real_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M1QkrH_31zK",
    "outputId": "6c6f4635-50bb-4a2d-8453-56f700ec6140"
   },
   "outputs": [],
   "source": [
    "real_test_pred          # 예측값들이 출력됨을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlxtDBDNNa6Y"
   },
   "source": [
    "## 7. Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn36fIuB42aM"
   },
   "outputs": [],
   "source": [
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "submission_file_path = os.path.join('output', f'{train_time}.csv')\n",
    "preds_df.to_csv(submission_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

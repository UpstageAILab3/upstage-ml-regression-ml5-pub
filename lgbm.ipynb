{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92WHLz8346Yf"
   },
   "source": [
    "# 서울 공동주택 실거래가 예측\n",
    "\n",
    "## Contents\n",
    "- Library Import\n",
    "- Data Load\n",
    "- Data Preprocessing\n",
    "- Feature Engineering\n",
    "- Model Training\n",
    "- Inference\n",
    "- Output File Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-DJzJjvO88V"
   },
   "source": [
    "## 1. Library Import\n",
    "- 필요한 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-NiCLGs4ZpM"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/font/NanumGothic.otf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPJvYT0OPAWS"
   },
   "source": [
    "## 2. Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwnwnpNJeR1"
   },
   "source": [
    "#### 2.1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx6nk3N_4_Tk"
   },
   "outputs": [],
   "source": [
    "# 필요한 데이터를 load 하겠습니다. 경로는 환경에 맞게 지정해주면 됩니다.\n",
    "train_path = './data/train.csv'\n",
    "test_path  = './data/test.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6tWMND66vT8",
    "outputId": "e0ba702b-6662-4723-c5a1-600d7236e597"
   },
   "outputs": [],
   "source": [
    "# Train data와 Test data shape은 아래와 같습니다.\n",
    "print('Train data shape : ', train_df.shape, 'Test data shape : ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVjw5Cnz24nz"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X24Y1OIR2_oa"
   },
   "source": [
    "- 모델링 전에 데이터 내 결측치, 이상치 등을 제거하고 범주형과 연속형 변수를 살펴보도록 하겠습니다!\n",
    "- 먼저, 용이한 전처리를 위해 train과 test data를 합친 하나의 데이터로 진행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vltXVoeRhZiI"
   },
   "outputs": [],
   "source": [
    "# train/test 구분을 위한 칼럼을 하나 만들어 줍니다.\n",
    "train_df['is_test'] = 0\n",
    "test_df['is_test'] = 1\n",
    "concat = pd.concat([train_df, test_df])     # 하나의 데이터로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-iZAY9WxQLK",
    "outputId": "ee049806-cb94-4336-f376-6a5c6947f79a"
   },
   "outputs": [],
   "source": [
    "concat['is_test'].value_counts()      # train과 test data가 하나로 합쳐진 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA를 통해 사용 결정한 필드들만 뽑아서 새로운 DataFrame 생성\n",
    "selected_fields = [\n",
    "    '시군구', '번지', '본번', '부번', '도로명', '아파트명',\n",
    "    '전용면적(㎡)', '계약년월', '계약일', '층', '건축년도',\n",
    "    'target', \n",
    "    'is_test'\n",
    "    ]\n",
    "\n",
    "concat_selected_df = concat[selected_fields].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = concat_selected_df.rename(columns={'전용면적(㎡)':'전용면적'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df['본번'] = concat_selected_df['본번'].astype('str')\n",
    "concat_selected_df['부번'] = concat_selected_df['부번'].astype('str')\n",
    "\n",
    "concat_selected_df['target'] = concat_selected_df['target'].fillna(0)\n",
    "concat_selected_df['target'] = concat_selected_df['target'].astype('int64')\n",
    "\n",
    "concat_selected_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지하철 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subway_path = './data/train_with_subway_infos.csv'\n",
    "train_subway_df = pd.read_csv(train_subway_path)\n",
    "print(train_subway_df.shape)\n",
    "print(train_subway_df.info())\n",
    "print(train_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subway_path  = './data/test_with_subway_infos.csv'\n",
    "test_subway_df = pd.read_csv(test_subway_path)\n",
    "print(test_subway_df.shape)\n",
    "print(test_subway_df.info())\n",
    "print(test_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_subway_df = pd.concat([train_subway_df, test_subway_df], axis=0) \n",
    "print(total_subway_df.shape)\n",
    "print(total_subway_df.info())\n",
    "print(total_subway_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = pd.concat([concat_selected_df, total_subway_df], axis=1) \n",
    "print(concat_selected_df.shape)\n",
    "print(concat_selected_df.info())\n",
    "print(concat_selected_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 버스 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bus_path = './data/train_with_bus_infos.csv'\n",
    "train_bus_df = pd.read_csv(train_bus_path)\n",
    "print(train_bus_df.shape)\n",
    "print(train_bus_df.info())\n",
    "print(train_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bus_path  = './data/test_with_bus_infos.csv'\n",
    "test_bus_df = pd.read_csv(test_bus_path)\n",
    "print(test_bus_df.shape)\n",
    "print(test_bus_df.info())\n",
    "print(test_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bus_df = pd.concat([train_bus_df, test_bus_df], axis=0) \n",
    "print(total_bus_df.shape)\n",
    "print(total_bus_df.info())\n",
    "print(total_bus_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지하철 정보에서 이미 머지되었으므로 버스에서는 삭제\n",
    "del total_bus_df['좌표X_2']\n",
    "del total_bus_df['좌표Y_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_selected_df = pd.concat([concat_selected_df, total_bus_df], axis=1) \n",
    "print(concat_selected_df.shape)\n",
    "print(concat_selected_df.info())\n",
    "print(concat_selected_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXfXRevr3dfe"
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytuwazY43gwQ"
   },
   "source": [
    "- 이제 위에서 만든 파생변수들과 정제한 데이터를 기반으로 본격적으로 부동산 실거래가를 예측하는 모델링을 진행하겠습니다.\n",
    "- 모델링에는 LGBM을 이용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdHNkPO_3tSb"
   },
   "source": [
    "### 5.1. 변수 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCW2dTz12VMZ",
    "outputId": "5c81b9e1-0462-421e-cb77-6b056532e5cd"
   },
   "outputs": [],
   "source": [
    "continuous_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for column in concat_selected_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_selected_df[column]):\n",
    "        continuous_features.append(column)\n",
    "    else:\n",
    "        categorical_features.append(column)\n",
    "\n",
    "continuous_features.remove('target')\n",
    "continuous_features.remove('is_test')\n",
    "\n",
    "print(\"연속형 변수:\", continuous_features)\n",
    "print(\"범주형 변수:\", categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 범주형 변수 인코딩\n",
    "\n",
    "1. One-Hot Encoding\n",
    "- 설명: 범주형 변수를 이진 벡터로 변환합니다. 각 범주는 고유한 이진 열을 가지며, 해당 범주에 속하면 1, 그렇지 않으면 0을 가집니다.\n",
    "- 장점: 범주형 데이터의 모든 범주를 고유하게 표현합니다.\n",
    "- 단점: 범주가 많을 경우 차원이 매우 커질 수 있습니다.\n",
    "\n",
    "2. Label Encoding\n",
    "- 설명: 각 범주를 고유한 정수로 변환합니다.\n",
    "- 장점: 간단하고 메모리 효율적입니다.\n",
    "- 단점: 범주 간의 순서가 암시될 수 있어 순서가 없는 명목형 변수에는 부적절할 수 있습니다.\n",
    "\n",
    "3. Ordinal Encoding\n",
    "- 설명: 순서가 있는 범주형 변수를 순서에 맞게 정수로 변환합니다.\n",
    "- 장점: 순서형 변수에 적합합니다.\n",
    "\n",
    "4. Frequency Encoding\n",
    "- 설명: 각 범주를 해당 범주의 빈도로 변환합니다.\n",
    "- 장점: 고유한 범주가 많을 때 유용합니다.\n",
    "\n",
    "5. Target Encoding\n",
    "- 설명: 각 범주를 해당 범주의 타겟 변수 평균으로 변환합니다.\n",
    "- 장점: 특정 범주가 타겟 변수에 미치는 영향을 반영합니다.\n",
    "\n",
    "[선택 기준]\n",
    "- 고유 범주 수: 범주가 많을 경우 One-Hot Encoding은 차원이 커지므로 Frequency Encoding이나 Target Encoding이 더 적합할 수 있습니다.\n",
    "- 변수의 특성: 순서가 있는 경우 Ordinal Encoding, 순서가 없는 경우 One-Hot Encoding이나 Frequency Encoding이 적합합니다.\n",
    "- 모델의 특성: 트리 기반 모델(LightGBM, Random Forest 등)은 범주형 변수를 직접 처리할 수 있어 Label Encoding이 적합할 수 있습니다. 반면, 선형 모델에서는 One-Hot Encoding이 더 효과적일 수 있습니다\n",
    "\n",
    "[추천]\n",
    "- 부동산 예측 대회에서 자주 사용되는 범주형 변수 인코딩 방법은 One-Hot Encoding과 Target Encoding입니다.\n",
    "- 트리 기반 모델을 사용하는 경우, Label Encoding도 좋은 선택이 될 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수 인코딩\n",
    "\n",
    "# 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
    "label_encoders = {}\n",
    "\n",
    "# Implement Label Encoding\n",
    "for col in tqdm(categorical_features):\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Label-Encoding을 fit\n",
    "    lbl.fit(concat_selected_df[col].astype(str))\n",
    "    concat_selected_df[col] = lbl.transform(concat_selected_df[col].astype(str))\n",
    "    label_encoders[col] = lbl # 결과 분석시 원복을 위해 인코더를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_features:\n",
    "    print(f\"{col}: {concat_selected_df[col].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속형 변수 인코딩\n",
    "\n",
    "1. StandardScaler\n",
    "- 설명: 평균을 0, 분산을 1로 변환합니다.\n",
    "- 장점: 데이터가 정규 분포를 따를 때 효과적입니다.\n",
    "\n",
    "2. MinMaxScaler\n",
    "- 설명: 데이터를 0과 1 사이로 스케일링합니다.\n",
    "- 장점: 모든 피처의 값이 동일한 범위(0~1)로 변환되므로 비교적 큰 값과 작은 값의 차이를 줄여줍니다.\n",
    "\n",
    "3. RobustScaler\n",
    "- 설명: 중앙값(median)과 IQR(Interquartile Range)을 사용하여 스케일링합니다.\n",
    "- 장점: 이상치(outlier)에 덜 민감합니다.\n",
    "\n",
    "4. Log Transformation\n",
    "- 설명: 로그 변환을 통해 비대칭적인 분포를 정규화합니다.\n",
    "- 장점: 극단값의 영향을 줄이고 분포를 정규화합니다.\n",
    "\n",
    "5. PowerTransformer\n",
    "- 설명: 데이터의 분포를 더 정규 분포에 가깝게 변환합니다.\n",
    "- 장점: 데이터가 강한 비대칭성을 가지고 있을 때 효과적입니다.\n",
    "\n",
    "[선택 기준]\n",
    "- 데이터 분포: 데이터의 분포가 정규 분포에 가까울 경우 StandardScaler가 적합하고, 비대칭성이 클 경우 PowerTransformer나 Log Transformation이 더 적합할 수 있습니다.\n",
    "- 이상치(outlier): RobustScaler는 이상치에 덜 민감하므로, 이상치가 많을 경우 적합합니다.\n",
    "- 해석 가능성: MinMaxScaler는 스케일링 후에도 데이터의 상대적인 순서를 유지하므로, 해석이 중요한 경우 유용할 수 있습니다.\n",
    "\n",
    "[추천]\n",
    "- 부동산 예측 대회에서 일반적으로 RobustScaler나 PowerTransformer가 좋은 선택이 될 수 있습니다.\n",
    "- 이 둘은 이상치에 강하고 비대칭성을 처리하는 데 효과적이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 연속형 변수 인코딩\n",
    "scaler = RobustScaler()\n",
    "concat_selected_df[continuous_features] = scaler.fit_transform(concat_selected_df[continuous_features])\n",
    "\n",
    "# 4. 로그 변환 (양수 값에 대해서만 적용 가능)\n",
    "# for feature in continuous_features:\n",
    "#     if (concat_selected_df[feature] > 0).all():\n",
    "#         concat_selected_df[f'{feature}_log'] = np.log1p(concat_selected_df[feature])\n",
    "\n",
    "# # 5. 이산화 (예: '전용면적'을 구간으로 나누기)\n",
    "# concat_selected_df['전용면적_구간'] = pd.cut(concat_selected_df['전용면적'], bins=5, labels=['매우작음', '작음', '중간', '큼', '매우큼'])\n",
    "\n",
    "# # 6. 다항식 특성 (선택적)\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# poly_features = poly.fit_transform(concat_selected_df[continuous_features])\n",
    "# poly_features_names = poly.get_feature_names_out(continuous_features)\n",
    "# df_poly = pd.DataFrame(poly_features, columns=poly_features_names)\n",
    "# df = pd.concat([concat_selected_df, df_poly], axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "# print(df[continuous_features + [f'{feature}_log' for feature in continuous_features] + ['전용면적_구간']].head())\n",
    "# print(df_poly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "-mMaaemI6DCI",
    "outputId": "67139863-5f67-4288-fb9a-b6f5a57a7e86"
   },
   "outputs": [],
   "source": [
    "concat_selected_df.head(1)        # 인코딩이 된 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 다시 train과 test dataset을 분할해줍니다. 위에서 제작해 놓았던 is_test 칼럼을 이용합니다.\n",
    "dt_train = concat_selected_df.query('is_test==0')\n",
    "dt_test = concat_selected_df.query('is_test==1')\n",
    "\n",
    "# 이제 is_test 칼럼은 drop해줍니다.\n",
    "dt_train.drop(['is_test'], axis = 1, inplace=True)\n",
    "dt_test.drop(['is_test'], axis = 1, inplace=True)\n",
    "print(dt_train.shape, dt_test.shape)\n",
    "\n",
    "dt_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4sHljC3NWje"
   },
   "source": [
    "### 5.2. Model Training\n",
    "- 위 데이터를 이용해 모델을 train 해보겠습니다. 모델은 LightGBM 이용하겠습니다.\n",
    "- Train과 Valid dataset을 분할하는 과정에서는 `holdout` 방법을 사용하겠습니다. 이 방법의 경우  대략적인 성능을 빠르게 확인할 수 있다는 점에서 baseline에서 사용해보도록 하겠습니다.\n",
    "  - 이 후 추가적인 eda를 통해서 평가세트와 경향을 맞추거나 kfold와 같은 분포에 대한 고려를 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S-ow8vVP_YZ"
   },
   "outputs": [],
   "source": [
    "assert dt_train.shape[1] == dt_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXP9IzrZaBMG"
   },
   "outputs": [],
   "source": [
    "# Target과 독립변수들을 분리해줍니다.\n",
    "y_train = dt_train['target']\n",
    "X_train = dt_train.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out split을 사용해 학습 데이터와 검증 데이터를 8:2 비율로 나누겠습니다.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGNHMAD2aBGI",
    "outputId": "5471313e-6838-44d3-8567-940c7456ef9e"
   },
   "outputs": [],
   "source": [
    "# LightGBM 모델 정의\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = os.path.join('model', f'{train_time}.pkl')\n",
    "\n",
    "# 학습된 모델을 저장합니다. Pickle 라이브러리를 이용하겠습니다.\n",
    "with open(model_file_path, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "- 트리 기반 모델의 feature_importances_ 값을 확인하여 가장 중요도가 높은 특성을 식별합니다.\n",
    "- 이 값들은 모델이 학습하는 과정에서 얼마나 자주 사용되었는지를 보여줍니다.\n",
    "- 중요한 특성들을 기반으로 특성 선택이나 모델 튜닝을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "VbgCRxFgdFQb",
    "outputId": "f9114f72-78e1-471c-fc0b-ba5b8b6b6d2f"
   },
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "feature_names = model.feature_name_\n",
    "\n",
    "sorted_features = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_features:\n",
    "    print(f'{name}: {importance}')\n",
    "\n",
    "feature_names_series = pd.Series([x[0] for x in sorted_features], name='name')\n",
    "importance_series = pd.Series([x[1] for x in sorted_features], name='importance')\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Feature Importances\")\n",
    "sns.barplot(x=importance_series, y=feature_names_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importance\n",
    "\n",
    "- Permutation Importance는 모델이 예측할 때 각 특성이 실제로 얼마나 중요한지를 평가합니다.\n",
    "- 이를 통해 각 특성이 모델 예측의 정확도에 미치는 영향을 더 직관적으로 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance 계산\n",
    "perm = PermutationImportance(model,\n",
    "                             scoring = 'neg_mean_squared_error', # 평가 지표로는 회귀문제이기에 negative rmse를 사용합니다. (neg_mean_squared_error : 음의 평균 제곱 오차)\n",
    "                             random_state = 42,\n",
    "                             n_iter=3).fit(X_val, y_val)\n",
    "\n",
    "eli5.show_weights(perm, feature_names = X_val.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = eli5.explain_weights_df(perm, feature_names = X_val.columns.tolist())\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='weight', y='feature', data=importance_df.sort_values(by='weight', ascending=False))\n",
    "plt.title('Permutation Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대한 예측\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측\n",
    "pred = model.predict(X_val)\n",
    "pred = pred.astype(int) # 하락시기라서 일단 버림으로\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YncDzsK1zl2w"
   },
   "source": [
    "### 5.4. 평가 및 분석\n",
    "\n",
    "- 훈련, 검증 세트에 대해 모델을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import evaluator\n",
    "\n",
    "# get [description, rmse, r2, mae]\n",
    "train_result = evaluator.evaluate_set(y_train, y_train_pred, \"Train\")\n",
    "val_result = evaluator.evaluate_set(y_val, pred, \"Valid\")\n",
    "\n",
    "comprehensive_report = evaluator.comprehensive_evaluation(train_result, val_result)\n",
    "print(comprehensive_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 인코딩 원복 \n",
    "for column in categorical_features :\n",
    "    X_val[column] = label_encoders[column].inverse_transform(X_val[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연속형 인코딩 원복\n",
    "X_val[continuous_features] = scaler.inverse_transform(X_val[continuous_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xViv0o6DXQ-"
   },
   "outputs": [],
   "source": [
    "# 분석을 위해 Validation dataset에 target과 pred 값, 두 값의 사이의 오류치를 채워주도록 하겠습니다.\n",
    "X_val['target'] = y_val\n",
    "X_val['pred'] = y_val_pred\n",
    "\n",
    "def calculate_error(target, pred):\n",
    "    squared_errors = (target - pred)\n",
    "    return squared_errors\n",
    "\n",
    "squared_errors = calculate_error(X_val['target'], X_val['pred'])\n",
    "X_val['error'] = squared_errors\n",
    "X_val['abs_error'] = np.abs(X_val['error'])\n",
    "X_val['error_rate'] = np.abs((X_val['target'] - X_val['pred']) / X_val['target'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'valid rmse : {val_result[1]}')\n",
    "\n",
    "threshold_absolute = val_result[1]  # 오차가 rmse 이상인 경우\n",
    "threshold_relative = 50  # 오차율이 x% 이상인 경우\n",
    "big_erros = X_val[(X_val['error'] >= threshold_absolute) & (X_val['error_rate'] >= threshold_relative)]\n",
    "big_erros.sort_values(by='error_rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_row = train_df.iloc[199547]\n",
    "error_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0Ga4ljBNYIy"
   },
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "7LC7NuRaD_Dd",
    "outputId": "c2827163-dbdd-4c8c-d35b-1c325b8d14c0"
   },
   "outputs": [],
   "source": [
    "dt_test.head(2)      # test dataset에 대한 inference를 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HXvn8ZSa1kt"
   },
   "outputs": [],
   "source": [
    "# 저장된 모델을 불러옵니다.\n",
    "with open(model_file_path, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbZ2A49LP_T9",
    "outputId": "89676c9b-c0a2-4951-84f0-430c5648331c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test = dt_test.drop(['target'], axis=1)\n",
    "\n",
    "# Test dataset에 대한 inference를 진행합니다.\n",
    "real_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M1QkrH_31zK",
    "outputId": "6c6f4635-50bb-4a2d-8453-56f700ec6140"
   },
   "outputs": [],
   "source": [
    "real_test_pred          # 예측값들이 출력됨을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlxtDBDNNa6Y"
   },
   "source": [
    "## 7. Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn36fIuB42aM"
   },
   "outputs": [],
   "source": [
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "submission_file_path = os.path.join('output', f'{train_time}.csv')\n",
    "preds_df.to_csv(submission_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
